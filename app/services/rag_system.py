import logging
import os
import chromadb
from sentence_transformers import SentenceTransformer
from app.core.config import settings

logger = logging.getLogger(__name__)

class RAGSystem:
    def __init__(self):
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAGSystem...")
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model_path = settings.EMBEDDING_MODEL_PATH if hasattr(settings, 'EMBEDDING_MODEL_PATH') else "intfloat/multilingual-e5-large"
        
        # –ï—Å–ª–∏ –ø—É—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(model_path) or os.path.exists(os.path.join(settings.BASE_DIR, "intfloat")):
             # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–∞–ø–∫—É intfloat –≤ –∫–æ—Ä–Ω–µ, –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –ø—É—Å—Ç–æ
             real_path = model_path if os.path.exists(model_path) else os.path.join(settings.BASE_DIR, "intfloat", "models--intfloat--multilingual-e5-large-instruct")
             logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {real_path}")
             self.model = SentenceTransformer(real_path)
        else:
             logger.info(f"üåê –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å {model_path}...")
             self.model = SentenceTransformer("intfloat/multilingual-e5-large")

        # 2. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        self.client = chromadb.PersistentClient(path=settings.CHROMA_PATH)
        self.collection = self.client.get_or_create_collection(name="library_collection")
        logger.info(f"‚úÖ RAG –ø–æ–¥–∫–ª—é—á–µ–Ω: {settings.CHROMA_PATH}")

    def add_document(self, text: str, source: str, title: str = "Unknown"):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç. –í–∞–∂–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å passage: –¥–ª—è E5"""
        # E5 –æ–∂–∏–¥–∞–µ—Ç "passage: " –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        content_to_embed = f"passage: {text}" 
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π!
        embedding = self.model.encode(content_to_embed, normalize_embeddings=True).tolist()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
        import hashlib
        doc_id = hashlib.md5((title + source + text[:50]).encode()).hexdigest()
        
        self.collection.upsert(
            ids=[doc_id],
            documents=[text], # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–±–µ–∑ passage:) –¥–ª—è —á—Ç–µ–Ω–∏—è
            metadatas=[{"source": source, "title": title}],
            embeddings=[embedding]
        )
    
    def add_book(self, book_data: dict):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –∫–Ω–∏–≥—É —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –≤ ChromaDB.
        
        Args:
            book_data: –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏:
                - title: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
                - author: –∞–≤—Ç–æ—Ä
                - subject: —Ä—É–±—Ä–∏–∫–∞/—Ç–µ–º–∞
                - grnti: –∫–æ–¥ –ì–†–ù–¢–ò
                - bbk: –∫–æ–¥ –ë–ë–ö
                - author_sign: –∞–≤—Ç–æ—Ä—Å–∫–∏–π –∑–Ω–∞–∫
                - systematic_code: —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —à–∏—Ñ—Ä
                - owners: –¥–µ—Ä–∂–∞—Ç–µ–ª—å (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
                - pdf_url: —Å—Å—ã–ª–∫–∞ –Ω–∞ PDF
                - pdf_ocr: —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–Ω–∏–≥–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        text_parts = [
            f"–ö–Ω–∏–≥–∞: {book_data.get('title', '')}",
            f"–ê–≤—Ç–æ—Ä: {book_data.get('author', '')}",
        ]
        
        if book_data.get('subject'):
            text_parts.append(f"–†—É–±—Ä–∏–∫–∞: {book_data.get('subject', '')}")
        
        if book_data.get('owners'):
            text_parts.append(f"–î–µ—Ä–∂–∞—Ç–µ–ª—å: {book_data.get('owners', '')}")
        
        if book_data.get('grnti'):
            text_parts.append(f"–ì–†–ù–¢–ò: {book_data.get('grnti', '')}")
        
        if book_data.get('bbk'):
            text_parts.append(f"–ë–ë–ö: {book_data.get('bbk', '')}")
        
        if book_data.get('systematic_code'):
            text_parts.append(f"–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —à–∏—Ñ—Ä: {book_data.get('systematic_code', '')}")
        
        if book_data.get('author_sign'):
            text_parts.append(f"–ê–≤—Ç–æ—Ä—Å–∫–∏–π –∑–Ω–∞–∫: {book_data.get('author_sign', '')}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –µ—Å—Ç—å
        if book_data.get('pdf_ocr'):
            text_parts.append(f"\n–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{book_data.get('pdf_ocr', '')}")
        
        text = "\n".join(text_parts)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        content_to_embed = f"passage: {text}"
        embedding = self.model.encode(content_to_embed, normalize_embeddings=True).tolist()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
        import hashlib
        doc_id = hashlib.md5((book_data.get('title', '') + book_data.get('author', '')).encode()).hexdigest()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ChromaDB
        self.collection.upsert(
            ids=[doc_id],
            documents=[text],
            metadatas=[{
                "title": book_data.get("title", ""),
                "author": book_data.get("author", ""),
                "subject": book_data.get("subject", ""),
                "grnti": book_data.get("grnti", ""),
                "bbk": book_data.get("bbk", ""),
                "author_sign": book_data.get("author_sign", ""),
                "systematic_code": book_data.get("systematic_code", ""),
                "owners": book_data.get("owners", ""),
                "pdf_url": book_data.get("pdf_url", ""),
            }],
            embeddings=[embedding]
        )
        
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–Ω–∏–≥–∞: {book_data.get('title', 'Unknown')[:50]}...")

    def search(self, query: str, top_k: int = 5) -> str:
        """–ü–æ–∏—Å–∫. –í–∞–∂–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å query: –¥–ª—è E5"""
        # E5 –æ–∂–∏–¥–∞–µ—Ç "query: " –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        query_to_embed = f"query: {query}"
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π!
        query_vec = self.model.encode(query_to_embed, normalize_embeddings=True).tolist()
        
        results = self.collection.query(
            query_embeddings=[query_vec],
            n_results=top_k
        )
        
        context = ""
        if results and results['documents']:
            for i, doc in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i]
                context += f"\n[–ò—Å—Ç–æ—á–Ω–∏–∫: {meta.get('title', '–ö–Ω–∏–≥–∞')}]\n{doc}\n"
                
        if not context:
            return "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
            
        return context
        
    def search_flexible(self, query: str, top_k: int = 5) -> str:
        """
        –ì–∏–±–∫–∏–π –ø–æ–∏—Å–∫ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏.
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "–ì–∞–≥–∞—Ä–∏–Ω –Æ.–ê."
        """
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫
        query_variants = [f"query: {query}"]
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ—Ä–æ—Ç–∫–∏–π (–≤–æ–∑–º–æ–∂–Ω–æ, –∏–º—è –∞–≤—Ç–æ—Ä–∞)
        if len(query.split()) <= 3:
            query_variants.append(f"query: –∞–≤—Ç–æ—Ä {query}")
            query_variants.append(f"query: –∫–Ω–∏–≥–∞ –∞–≤—Ç–æ—Ä–∞ {query}")
            
            # –£–±–∏—Ä–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª—ã –∏ —Ç–æ—á–∫–∏: "–ì–∞–≥–∞—Ä–∏–Ω –Æ.–ê." -> "–ì–∞–≥–∞—Ä–∏–Ω"
            cleaned = query.split()[0] if ' ' in query else query.replace('.', '').strip()
            if cleaned != query:
                query_variants.append(f"query: {cleaned}")
        
        # –ò—â–µ–º –ø–æ –≤—Å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_results = {}
        
        for variant in query_variants:
            query_vec = self.model.encode(variant, normalize_embeddings=True).tolist()
            
            results = self.collection.query(
                query_embeddings=[query_vec],
                n_results=top_k
            )
            
            if results and results['documents']:
                for i, doc in enumerate(results['documents'][0]):
                    doc_id = results['ids'][0][i] if 'ids' in results else str(i)
                    if doc_id not in all_results:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                        meta = results['metadatas'][0][i]
                        score = results['distances'][0][i] if 'distances' in results else 0
                        all_results[doc_id] = {
                            'doc': doc,
                            'meta': meta,
                            'score': score
                        }
        
        if not all_results:
            return "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        sorted_results = sorted(all_results.values(), key=lambda x: x['score'])[:top_k]
        
        context = ""
        for item in sorted_results:
            meta = item['meta']
            doc = item['doc']
            context += f"\n[–ò—Å—Ç–æ—á–Ω–∏–∫: {meta.get('title', '–ö–Ω–∏–≥–∞')}]\n{doc}\n"
        
        return context